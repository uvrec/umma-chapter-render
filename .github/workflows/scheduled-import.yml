name: Scheduled Import

on:
  schedule:
    - cron: '0 2 * * *'  # daily at 02:00 UTC
  workflow_dispatch:
    inputs:
      start_urls:
        description: 'URLs to import (comma-separated)'
        required: false
        default: 'https://vedabase.io/en/library/bg/,https://gitabase.com/ukr/bg/'
      worker_concurrency:
        description: 'Number of parallel workers'
        required: false
        default: '3'
      dry_run:
        description: 'Dry run mode (preview only)'
        required: false
        type: boolean
        default: false

env:
  NODE_VERSION: '18'
  ARTIFACT_DIR: ci_artifacts
  PG_CONNECTION: ${{ secrets.PG_CONNECTION_staging }} # default to staging; change per env if needed

jobs:
  pre-import-check:
    name: Pre-Import Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      can_proceed: ${{ steps.validate.outputs.can_proceed }}
      has_duplicates: ${{ steps.check_dups.outputs.has_duplicates }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Validate DB connectivity
        id: validate
        env:
          PG_CONNECTION: ${{ secrets.PG_CONNECTION_staging }}
        run: |
          set -euo pipefail
          if psql "$PG_CONNECTION" -c "SELECT 1;" >/dev/null 2>&1; then
            echo "can_proceed=true" >> $GITHUB_OUTPUT
          else
            echo "can_proceed=false" >> $GITHUB_OUTPUT
            echo "::error::Cannot connect to DB"
            exit 1
          fi

      - name: Quick duplicate check
        id: check_dups
        env:
          PG_CONNECTION: ${{ secrets.PG_CONNECTION_staging }}
        run: |
          set -euo pipefail
          DUP_COUNT=$(psql "$PG_CONNECTION" -t -A -c "
            SELECT COUNT(*) FROM (
              SELECT chapter_id, verse_number FROM public.verses
              GROUP BY chapter_id, verse_number HAVING COUNT(*) > 1
            ) t;
          " | tr -d '[:space:]' || echo "0")
          if [ -z "$DUP_COUNT" ]; then
            DUP_COUNT=0
          fi
          echo "Found duplicate verse groups: $DUP_COUNT"
          if [ "$DUP_COUNT" -gt 0 ]; then
            echo "has_duplicates=true" >> $GITHUB_OUTPUT
            echo "duplicate_count=$DUP_COUNT" >> $GITHUB_OUTPUT
          else
            echo "has_duplicates=false" >> $GITHUB_OUTPUT
            echo "duplicate_count=0" >> $GITHUB_OUTPUT
          fi

  run-import:
    name: Run Import
    needs: pre-import-check
    runs-on: ubuntu-latest
    if: needs.pre-import-check.outputs.can_proceed == 'true'
    timeout-minutes: 120

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps chromium

      - name: Build project
        run: npm run build

      - name: Run import
        id: import
        env:
          START_URLS: ${{ github.event.inputs.start_urls || 'https://vedabase.io/en/library/bg/,https://gitabase.com/ukr/bg/' }}
          WORKER_CONCURRENCY: ${{ github.event.inputs.worker_concurrency || '3' }}
          PG_CONNECTION: ${{ secrets.PG_CONNECTION_staging }}
        run: |
          set -euo pipefail
          mkdir -p $ARTIFACT_DIR
          echo "Starting import. Dry run=${{ github.event.inputs.dry_run }}"
          if [ "${{ github.event.inputs.dry_run }}" = "true" ]; then
            echo "DRY RUN - no writes will be performed" | tee $ARTIFACT_DIR/import.log
            # If your import supports dry-run, pass env var or flag to it
            # Example: npm run import:start -- --dry-run
          else
            npm run import:start 2>&1 | tee $ARTIFACT_DIR/import.log || true
          fi

          # Determine basic stats from log (best-effort)
          SAVED=$(grep -c "✅ Saved:" $ARTIFACT_DIR/import.log || true)
          FAILED=$(grep -c "❌ Failed:" $ARTIFACT_DIR/import.log || true)
          SKIPPED=$(grep -c "⏭️  Skipped:" $ARTIFACT_DIR/import.log || true)

          echo "saved=$SAVED" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT
          echo "skipped=$SKIPPED" >> $GITHUB_OUTPUT

      - name: Upload import log
        uses: actions/upload-artifact@v4
        with:
          name: import-log-${{ github.run_number }}
          path: ${{ env.ARTIFACT_DIR }}/import.log
          retention-days: 30

  post-import-validation:
    name: Post-Import Validation
    runs-on: ubuntu-latest
    needs: run-import
    timeout-minutes: 20
    if: always()

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Validate data integrity
        env:
          PG_CONNECTION: ${{ secrets.PG_CONNECTION_staging }}
        run: |
          set -euo pipefail
          mkdir -p $ARTIFACT_DIR

          ORPHANS=$(psql "$PG_CONNECTION" -t -A -c "
            SELECT COUNT(*) FROM public.verses v
            WHERE NOT EXISTS (SELECT 1 FROM public.chapters c WHERE c.id = v.chapter_id);
          " | tr -d '[:space:]' || echo "0")

          MISSING_UA=$(psql "$PG_CONNECTION" -t -A -c "
            SELECT COUNT(*) FROM public.verses WHERE translation_ua IS NULL;
          " | tr -d '[:space:]' || echo "0")

          MISSING_EN=$(psql "$PG_CONNECTION" -t -A -c "
            SELECT COUNT(*) FROM public.verses WHERE translation_en IS NULL;
          " | tr -d '[:space:]' || echo "0")

          echo "orphans=$ORPHANS" >> $GITHUB_OUTPUT
          echo "missing_ua=$MISSING_UA" >> $GITHUB_OUTPUT
          echo "missing_en=$MISSING_EN" >> $GITHUB_OUTPUT

      - name: Generate quality report
        env:
          PG_CONNECTION: ${{ secrets.PG_CONNECTION_staging }}
        run: |
          cat > $ARTIFACT_DIR/quality_report.sql <<'SQL'
          -- Data completeness by book
          SELECT
            b.slug,
            COUNT(v.id) as total_verses,
            COUNT(v.translation_ua) as has_ua,
            COUNT(v.translation_en) as has_en,
            ROUND(100.0 * COUNT(v.translation_ua) / NULLIF(COUNT(v.id),0), 1) as ua_pct,
            ROUND(100.0 * COUNT(v.translation_en) / NULLIF(COUNT(v.id),0), 1) as en_pct
          FROM public.books b
          JOIN public.chapters c ON c.book_id = b.id OR c.canto_id IN (SELECT id FROM public.cantos WHERE book_id = b.id)
          JOIN public.verses v ON v.chapter_id = c.id
          GROUP BY b.slug
          ORDER BY total_verses DESC;
          SQL

          psql "$PG_CONNECTION" -f $ARTIFACT_DIR/quality_report.sql > $ARTIFACT_DIR/quality_report.txt 2>&1 || true
          cat $ARTIFACT_DIR/quality_report.txt

      - name: Upload quality report
        uses: actions/upload-artifact@v4
        with:
          name: quality-report-${{ github.run_number }}
          path: ${{ env.ARTIFACT_DIR }}/quality_report.txt
          retention-days: 90

  notify:
    name: Send Notifications
    runs-on: ubuntu-latest
    needs: [run-import, post-import-validation]
    if: always()
    steps:
      - name: Prepare notification summary
        id: prepare
        run: |
          echo "status=Completed" >> $GITHUB_OUTPUT

      - name: Notify Slack (optional)
        if: secrets.SLACK_WEBHOOK_URL
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          python3 - <<'PY'
import os, requests, json
url=os.environ['SLACK_WEBHOOK_URL']
text=f"Import run #${{ github.run_number }} completed for repo ${{ github.repository }}."
requests.post(url, json={"text": text})
PY

  cleanup:
    name: Cleanup Old Artifacts
    runs-on: ubuntu-latest
    needs: notify
    if: always()
    steps:
      - name: Delete old artifacts (30 days)
        uses: actions/github-script@v7
        with:
          script: |
            const days = 30;
            const cutoff = Date.now() - (days * 24 * 60 * 60 * 1000);
            const artifacts = await github.rest.actions.listArtifactsForRepo({ owner: context.repo.owner, repo: context.repo.repo, per_page: 100 });
            for (const a of artifacts.data.artifacts) {
              if (a.name.startsWith('import-log-') && (new Date(a.created_at)).getTime() < cutoff) {
                await github.rest.actions.deleteArtifact({ owner: context.repo.owner, repo: context.repo.repo, artifact_id: a.id });
                console.log('Deleted artifact', a.name);
              }
            }
